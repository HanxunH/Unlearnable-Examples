---
headless: true
weight: 20

title: "Unlearnable Examples: Making Personal Data Unexploitable"
subtitle: ""

design:
  view: 3
  columns: "1"
---

<div class="article-metadata">
  <div>
    <span><a href="https://hanxunh.github.io/">Hanxun Huang</a></span>, 
    <span><a href="http://xingjunma.com/">Xingjun Ma</a></span>, 
    <span><a href="https://people.eng.unimelb.edu.au/smonazam/">Sarah Monazam Erfani</a></span>, 
    <span><a href="https://people.eng.unimelb.edu.au/baileyj/">James Bailey</a></span>, 
    <span><a href="https://yisenwang.github.io/">Yisen Wang</a></span>
  </div>
  <span class="article-date">May 2021</span>
</div>

<div class="btn-links mb-3">
  <a class="btn btn-outline-primary btn-page-header" href="https://openreview.net/pdf?id=iAmZUo0DxC0" target="_blank" rel="noopener">PDF</a>
  <a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename="/Unlearnable-Examples/publication/conference-paper/cite.bib">Cite</a>
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/HanxunH/Unlearnable-Examples" target="_blank" rel="noopener">Code</a>
</div>

<!-- Header image -->
![](/Unlearnable-Examples/publication/conference-paper/header.jpg)

### Abstract
The volume of "free" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is "nothing" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important Ô¨Årst step towards making personal data unexploitable to deep learning models.


<!-- Add more text here -->





<!-- Share Area: DO NOT CHANGE -->
<div class="share-box" aria-hidden="true"><ul class="share"><li><a href="https://twitter.com/intent/tweet?url=https://hanxunh.github.io/Unlearnable-Examples/publication/conference-paper/&amp;text=Unlearnable%20Examples:%20Making%20Personal%20Data%20Unexploitable" target="_blank" rel="noopener" class="share-btn-twitter"><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://hanxunh.github.io/Unlearnable-Examples/publication/conference-paper/&amp;t=Unlearnable%20Examples:%20Making%20Personal%20Data%20Unexploitable" target="_blank" rel="noopener" class="share-btn-facebook"><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Unlearnable%20Examples:%20Making%20Personal%20Data%20Unexploitable&amp;body=https://hanxunh.github.io/Unlearnable-Examples/publication/conference-paper/" target="_blank" rel="noopener" class="share-btn-email"><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://hanxunh.github.io/Unlearnable-Examples/publication/conference-paper/&amp;title=Unlearnable%20Examples:%20Making%20Personal%20Data%20Unexploitable" target="_blank" rel="noopener" class="share-btn-linkedin"><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Unlearnable%20Examples:%20Making%20Personal%20Data%20Unexploitable%20https://hanxunh.github.io/Unlearnable-Examples/publication/conference-paper/" target="_blank" rel="noopener" class="share-btn-whatsapp"><i class="fab fa-whatsapp"></i></a></li></ul></div>
