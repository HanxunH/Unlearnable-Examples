<!doctype html><html lang><head><meta name=generator content="Hugo 0.83.1"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Unlearnable Examples: Making Personal Data Unexploitable</title><meta name=description content="A simple monospaced resume theme for Hugo."><meta name=author content="Hanxun Huang"><link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin=anonymous><link rel=stylesheet href=/Unlearnable-Examples/sass/researcher.min.css><link rel=icon type=image/ico href=https://hanxunh.github.io/Unlearnable-Examples/favicon.ico><link rel=alternate type=application/rss+xml href=https://hanxunh.github.io/Unlearnable-Examples/index.xml title="Unlearnable Examples: Making Personal Data Unexploitable"></head><body><div class="container mt-5"><nav class="navbar navbar-expand-sm flex-column flex-sm-row text-nowrap p-0"><a class="navbar-brand mx-0 mr-sm-auto" href=https://hanxunh.github.io/Unlearnable-Examples/ title="Unlearnable Examples: Making Personal Data Unexploitable">Unlearnable Examples: Making Personal Data Unexploitable</a><div class="navbar-nav flex-row flex-wrap justify-content-center"><a class="nav-item nav-link" href="https://openreview.net/pdf?id=iAmZUo0DxC0" title="Download Paper">Download Paper</a>
<span class="nav-item navbar-text mx-1">/</span>
<a class="nav-item nav-link" href=https://github.com/HanxunH/Unlearnable-Examples title=Code>Code</a></div></nav></div><hr><div id=content><div class=container><h2 id=we-need-more-control-of-how-our-data-is-used>We need more control of how our data is used.</h2><p>The volume of &ldquo;free&rdquo; data on the internet has been critical to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. We believe it&rsquo;s crucial to develop methods to allow individuals to take active actions and prevent their data from any unauthorized exploitations.</p><p>In this research, we present a type of <em>error-minimizing (unlearnable) noise</em> that can make training examples unlearnable to deep learning. <em>The unlearnable noise can be used by an individual to tag their data, so that it canâ€™t easily be used by others for training their machine learning systems. This gives the owner more control about how their data is used.</em></p><hr><h4 id=what-are-unlearnable-examples>What are Unlearnable Examples?</h4><p><em>AI is supposed to learn from its own mistakes (errors). But what happens if there are no mistakes or it is too easy to learn, will learning stop?</em></p><p><em>Unlearnable examples</em> exploits the above aspect of AI and tricks the model to believe <em>there is nothing to learn</em>. Deep Neural Networks (DNNs) trained on <em>unlearnable examples</em> will have a performance equivalent to random guessing on normal test examples.</p><table><tr><td style=border:none><img src=images/fig1_classwise.jpg width=375></td><td style=border:none><img src=images/fig1_samplewise.jpg width=375></td></tr></table><p>The unlearnable effectiveness of different types of noise: random, adversarial (error-maximizing) and our proposed error-minimizing noise on CIFAR-10 dataset. The lower the clean test accuracy the more effective of the noise.</p><h4 id=difference-to-adversarial-examples>Difference to adversarial examples?</h4><p>Adversarial examples can protect you from being recognized by a well-trained model (the model learns the version A of your data, while adversarial examples are version B), while unlearnable examples protect your data from contributing to any model training.</p><h4 id=how-to-use-unlearnable-noise-to-protect-your-data>How to use unlearnable noise to protect your data?</h4><p>Before you release your data to the wild, add an imperceptible noise to the data to create unlearnable data. We need to pre-generate the noise based on a public dataset, based on image categories (classes). Then you can choose the category-specific noise to add to your image according to its category. The noise can also be generated for each image individually (sample-wise noise).</p><p>We plan to develop an app for you to use in the future.</p><p><img src=images/exp_face_authors.png alt></p><hr><h2 id=examples-on-cifar-10>Examples on CIFAR-10</h2><p>In our code repository, we have a <a href=https://github.com/HanxunH/Unlearnable-Examples/blob/main/QuickStart.ipynb>QuickStart notebook</a> that contains minimal implementations for sample-wise error-minimizing noise.</p><p><img src=images/CIFAR-10-example.png alt>
This is an example of Unlearnable Examples on CIFAR-10.
From left to right: Original Images, Visualization of the Error-Minimizing Noise and Unlearnable Images.</p><hr><h2 id=iclr-2021-poster>ICLR-2021 Poster</h2><p><img src=images/poster.jpg alt></p><hr><h2 id=researchers>Researchers</h2><ul><li><a href=https://hanxunh.github.io/>Hanxun Huang</a> , PhD student, The University of Melbourne</li><li><a href=http://xingjunma.com/>Xingjun Ma</a> , Lecturer, Deakin University</li><li><a href=https://people.eng.unimelb.edu.au/smonazam/>Sarah Erfani</a> , Senior Lecturer, The University of Melbourne</li><li><a href=https://people.eng.unimelb.edu.au/baileyj/>James Bailey</a> , Professor, The University of Melbourne</li><li><a href=https://yisenwang.github.io/>Yisen Wang</a> , Assistant Professor, Peking University</li></ul><p><img src=images/unlearnable_authors.jpeg alt></p><ul><li>Top row: original photos;</li><li>Bottom row: unlearnable photos generated using our technology.</li></ul><h2 id=media-coverage>Media Coverage</h2><ul><li>Pursuit: <a href=https://pursuit.unimelb.edu.au/articles/blocking-ai-to-keep-your-personal-data-your-own>Blocking AI to keep your personal data your own</a></li><li>Gadgets 360: <a href=https://gadgets.ndtv.com/science/news/facial-recognition-ai-spoofing-for-selfies-fawkes-lowkey-image-cloaking-adversarial-attacks-2439007>Worried About Privacy for Your Selfies? These Tools Can Help Spoof Facial Recognition AI</a></li><li>MIT Technology Review: <a href=https://www.technologyreview.com/2021/05/05/1024613/stop-ai-recognizing-your-face-selfies-machine-learning-facial-recognition-clearview/>How to stop AI from recognizing your face in selfies</a></li></ul><hr><h2 id=cite-our-work>Cite Our Work</h2><pre><code>@inproceedings{huang2021unlearnable,
    title={Unlearnable Examples: Making Personal Data Unexploitable},
    author={Hanxun Huang
      and Xingjun Ma
      and Sarah Monazam Erfani
      and James Bailey
      and Yisen Wang},
    booktitle={ICLR},
    year={2021}
}
</code></pre></div></div><div id=footer class=mb-5><hr><div class="container text-center"></div><div class="container text-center"><a href=http://hanxunh.github.io/ title="Hanxun Huang"><small>Hanxun Huang</small></a></div></div></body></html>